---
layout: post
title:  "밴디트 문제(バンディット問題)"
summary: 밴디트 문제(バンディット問題)
author: YUN JAEYEONG
date: '2026-01-27 19:00:00 +0900'
category: [강화학습, 強化学習]
typora-root-url: ../
use_math: true
#thumbnail: /assets/img/posts/code.jpg
---
 <script type="text/javascript" async
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>
## 밴디트 문제(バンディット問題)

### 머신러닝의 분류（機械学習の分類）

1. 지도 학습:정답이 있는 데이터를 활용해 데이터를 학습

2. 비지도 학습:정답 레이블이 없는 데이터를 군집화 하는 학습

3. 강화 학습:에이전트가 환경과 상호작용하면서 수집한 데이터를 바탕으로 더 많은 보상을 얻는 방법 학습

   ㅤ

1. 教師あり学習：正解ラベル付きデータを用いて学習する手法
2. 教師なし学習：正解ラベルのないデータをクラスタリングする学習
3. 強化学習：エージェントが環境と相互作用しながら得たデータを基に，より多くの報酬を得る方法を学習する手法

### 밴디트 문제（バンディット問題）

슬롯머신 = 환경

スロットマシン＝環境

플레이어 = 에이전트

プレイヤー＝エージェント

### 좋은 슬롯 머신이란?（良いスロットマシンとは？）

기댓값(expectation value)이 높은 슬롯 머신이 더 좋은 슬롯머신이다.

期待値が高いスロットマシンほど、いいスロットマシンである。

#### 평균을 구하는 코드（平均を求めるコード）

$$
Q_n = \frac{R_1 + R_2 + \cdots + R_n} {n}
$$

```python
#avg.py
import numpy as np

np.random.seed(0)
rewards=[]
for n in range(1, 11):
    reward = np.random.rand()
    rewards.append(reward)
    Q = sum(rewards) / n
    print(Q)
```



#### 증분 구현(incremental implementation、インクリメンタルに計算)


$$
Q_{n-1} = \frac{R_1 + R_2 + \cdots + R_{n-1}} {n-1}
$$

$$
R_1 + R_2 + \cdots + R_{n-1} = (n-1)Q_{n-1}
$$

$$
Q_n = \frac{R_1 + R_2 + \cdots + R_n} {n}
\\= \frac{1}{n}(R_1+\cdots+R_{n-1}+R_n)
\\= \frac{1}{n}\{(n-1)Q_{n-1}+R_n\}
\\= (1-\frac{1}{n})Q_{n-1} + \frac{1}{n}R_n
$$

$$
Q_n = (1-\frac{1}{n})Q_{n-1} + \frac{1}{n}R_n
\\=Q_{n-1} + \frac{1}{n}(R_n-Q_{n-1})
$$



![Qn-1,Qn,Rn의 위치 관계](/assets/images/rl/1.bandit/1.Qn-1,Qn,Rn의 위치 관계.jpg)

```python
Q = 0
np.random.seed(0)

for n in range(1, 11):
    reward = np.random.rand()
    #Q = Q + (reward - Q) / n
    Q += (reward - Q) / n
    print(Q)
```



#### 플레이어의 정책（プレイヤーのポリシー）

활용:지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이(탐욕 정책)

탐색:슬롯머신의 가치를 정확하게 추정하기 위해 다양한 슬롯머신을 시도

活用：これまでの実際のプレイ結果を基に、最もいいと考えられるスロットマシンを選択する（貪欲方策）

探索：スロットマシンの価値を正確に推定するため、様々なスロットマシンを試す

##### Epsilon-Greedy Policy

ϵ의 확률로 '탐색'을 하고, 1-ϵ의 확률로 '활용'을 하는 정책.

確率ϵで「探索」を行い、確率1-ϵで「活用」を行う方策



### 밴디트 알고리즘의 구현（バンディットアルゴリズムの実装）

```python
#bandit.py
import numpy as np

class Bandit:
    def __init__(self, arms=10):
        self.rates = np.random.rand(arms)
    def play(self, arm):
        rate = self.rates[arm]
        if rate > np.random.rand():
            return 1
        else:
            return 0
```

```python
bandit = Bandit()

for i in range(3):
    print(bandit.play(0))
```

```python
bandit = Bandit()
Q = 0

for n in range(1, 11):
    reward = bandit.play(0)
    Q += (reward - Q) / n
    print(Q)
```

```python
bandit = Bandit()
Qs = np.zeros(10)
ns = np.zeros(10)

for n in range(10):
    action = np.random.randint(0, 10)
    reward = bandit.play(action)
    ns[action] += 1
    Qs[action] += (reward - Qs[action]) / ns[action]
    print(Qs)
```

### 에이전트 구현（エージェントの実装）

```
class Agent:
    def __init__(self, epsilon, action_size=10):
        self.epsilon = epsilon
        self.Qs = np.zeros(action_size)
        self.ns = np.zeros(action_size)

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]

    def get_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(0, len(self.Qs))
        return np.argmax(self.Qs)
```

### 실행해보기（実行してみる）

```python
import matplotlib.pyplot as plt

steps = 1000
epsilon = 0.1

bandit = Bandit()
agent = Agent(epsilon)
total_reward = 0
total_rewards = []
rates = []

for step in range(steps):
    action = agent.get_action()
    reward = bandit.play(action)
    agent.update(action, reward)
    total_reward += reward
    
    total_rewards.append(total_reward)
    rates.append(total_reward / (step + 1))

print(total_reward)

plt.ylabel('Total reward')
plt.xlabel('Steps')
plt.plot(total_rewards)
plt.show()

plt.ylabel('Rates')
plt.xlabel('Steps')
plt.plot(rates)
plt.show()
```

![2.단계에 따른 보상 총합](/assets/images/rl/1.bandit/2.단계에 따른 보상 총합.png)

![3.단계별 승률](/assets/images/rl/1.bandit/3.단계별 승률.png)



```
#bandit_avg.py
runs = 200
steps = 1000
epsilon = 0.1
all_rates = np.zeros((runs, steps))
for run in range(runs):
    bandit = Bandit()
    agent = Agent(epsilon)
    total_reward = 0
    rates = []

    for step in range(steps):
        action = agent.get_action()
        reward = bandit.play(action)
        agent.update(action, reward)
        total_reward += reward
        rates.append(total_reward / (step + 1))

    all_rates[run] = rates

avg_rates = np.average(all_rates, axis=0)

plt.ylabel("Rates")
plt.xlabel('Steps')
plt.plot(avg_rates)
plt.show()
```

![4.단계별 승률(200번 실험 후 평균)](/assets/images/rl/1.bandit/4.단계별 승률(200번 실험 후 평균).png)

## 비정상 문제（非定常問題）

정상 문제(stationary problem):보상의 확률 분포가 변하지 않는 문제

비정상 문제(non-stationary problem):보상의 확률 분포가 변하도록 설정된 문제

定常問題：報酬の確率分布が変化しない問題

非定常問題：報酬の確率分布が時間とともに変化するように設定された問題

```python
#non-stationary 비정상문제
class NonStatBandit:
    def __init__(self, arms=10):
        self.arms = arms
        self.rates = np.random.rand(arms)

    def play(self, arm):
        rate = self.rates[arm]
        self.rates += 0.1 * np.random.randn(self.arms) #노이즈 추가
        if rate > np.random.rand():
            return 1
        else: 
            return 0
```

##### 지수 이동 평균(exponential moving average、指数移動平均)

$$
Q_n = \frac{R_1 + R_2 + \cdots + R_n} {n}
\\= \frac{1}{n}R_1+\frac{1}{n}R_2+\cdots+\frac{1}{n}R_n
$$



이 때 1/n을 각 보상에 대한 '가중치'로 볼 수 있다.

このとき、1/nを各報酬に対する「重み」とみなすことができる

![6.각 보상에 대한 가중치(표본 평균의 경우)](/assets/images/rl/1.bandit/6.각 보상에 대한 가중치(표본 평균의 경우).jpg)
$$
Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})
$$


여기서 1/n을 α(0 < α < 1)라는 고정값으로 바꾸면 각 보상의 가중치가 다음과 같이 변한다.

ここで1/nをα(0 < α < 1)という固定値に置き換えると、各報酬の重みは次のように変化する。


$$
Q_n = Q_{n-1} + \alpha(R_n - Q_{n-1})
$$
![7.각 보상에 대한 가중치(고정값알파)](/assets/images/rl/1.bandit/7.각 보상에 대한 가중치(고정값알파).jpg)
$$
Q_n = Q_{n-1} + \alpha(R_n - Q_{n-1})
\\= \alpha Rn + (1-\alpha)Q_{n-1}
$$

$$
Q_{n-1} = \alpha R_{n-1}+(1-\alpha)Q_{n-2}
$$

$$
Q_n = \alpha Rn + (1-\alpha)Q_{n-1}
\\= \alpha Rn + (1-\alpha)\{\alpha R_{n-1}+(1-\alpha)Q_{n-2}\}
\\= \alpha Rn + \alpha(1-\alpha)R_{n-1}+(1-\alpha)^2Q_{n-2}
$$



이 전개를 n번 반복하면 다음 식이 만들어진다.

この展開をｎ回繰り返すと次の式が得られる。
$$
Q_n = \alpha R_n + \alpha(1-\alpha)R_{n-1} +\cdots+\alpha(1-\alpha)^{n-1}R_1+\alpha(1-\alpha)^{n}Q_0
$$


이처럼 가중치가 지수적으로 감소하기 때문에 이를 지수 이동 평균이라 한다.

このように重みが指数的に減少するため、これを指数移動平均と呼ぶ。

#### 비정상 문제 풀기（非定常問題の解き方）

Agent의 추정치를 고정값 α로 갱신한다.

アージェントの推定値を固定値αを用いて更新する。

```python
#exponential moving average / exponential weighted moving average
class AlphaAgent:
    def __init__(self, epsilon, alpha, actions=10):
        self.epsilon = epsilon
        self.Qs = np.zeros(actions)
        self.alpha = alpha

    def update(self, action, reward):
        self.Qs[action] += (reward - self.Qs[action]) * self.alpha #고정값 α

    def get_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(0, len(self.Qs))
        return np.argmax(self.Qs)
```

![5.표본 평균과 고정값 α에 의한 갱신 비교](/assets/images/rl/1.bandit/5.표본 평균과 고정값 α에 의한 갱신 비교.png)

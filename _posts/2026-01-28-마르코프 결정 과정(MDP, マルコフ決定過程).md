---
layout: post
title:  "마르코프 결정 과정(MDP, マルコフ決定過程)"
summary: 마르코프 결정 과정(MDP, マルコフ決定過程)
author: YUN JAEYEONG
date: '2026-01-28 16:00:00 +0900'
category: [강화학습, 強化学習]
typora-root-url: ../
use_math: true
#thumbnail: /assets/img/posts/code.jpg

---
<script type="text/javascript" async
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>


## 마르코프 결정 과정(MDP, マルコフ決定過程)

### 결정 과정（決定過程）

'결정 과정'이란 '에이전트가 (환경과 상호작용하면서) 행동을 결정하는 과정'을 뜻한다.

에이전트가 행동을 취함으로써 상태가 변하며, 보상도 달라진다.

「決定過程」とは、「エージェントが（環境と相互作用しながら）行動を決定する過程」を指す。

エージェントが行動を取ることによって状態が変化し、それに伴って報酬も変化する。

![1.MDP의 사이클](/assets/images/rl/2.MDP/1.MDP의 사이클.jpg)
$$
S_0,A_0,R_0,S_1,S_1,R_1,S_2,A_2,R_2,\cdots
$$


에이전트와 환경의 상호작용은 위와 같은 전이를 만들어낸다.

エージェントと環境の相互作用は、上記のような遷移を生み出す

### 상태전이（状態遷移）

#### 상태 전이 함수(状態遷移関数)

$$
p(s'|s,a)
$$

위 식을 상태 전이 확률이라고 한다. 에이전트가 상태 s에서 행동 a를 선택했을 때, 다음 상태 s'로 이동할 확률이다.

上式を状態遷移確立という。エージェントが状態sにおいて行動aを選択したとき、次の状態s'に遷移する確率を表す。

##### 마르코프 성질（マルコフ性）

위 식에서 다음 상태 s'를 결정하는데 현재 상태와 행동인 s와 a만이 영향을 준다.

이처럼 마르코프 성질이란 현재의 정보만을 고려하는 성질이다.

"과거의 모든 이력은 현재 상태에 이미 반영되어 있다."는 가정이다.

MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이와 보상을 모델링한다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야해서, 그 조합이 기하급수적으로 많아진다.

上式において、次の状態s'を決定するのに影響を与えるのは、現在の状態と行動であるsとaのみである。

このように、マルコフ性とは「現在の情報のみを考慮する性質」を指す。

「過去のすべての履歴は、現在の状態にすでに反映されている」という仮定である。

MDPでは、マルコフ性が成り立つと仮定して状態遷移と報酬をモデリングする。もしマルコフ性を仮定しない場合、過去のすべての状態や行動まで考慮する必要があり、その組み合わせは幾何級数的に増大してしまう。



#### 보상 함수（報酬関数）

$$
r(s, a, s')
$$

위 식을 보상 함수라 한다. 에이전트가 상태 s에서 행동 a를 수행하여 다음 상태가 s'가 되었을 떄 얻는 보상을 의미한다.

上式を報酬関数という。エージェントが状態sにおいて行動aを実行し、次の状態がs'になったときに得られる報酬を意味する。

#### 에이전트의 정책（エージェントの方策）

$$
\pi(a|s)
$$

위 식은 상태 s에서 행동 a를 취할 확률을 나타낸다.

上式は、状態 sにおいて行動 aを取る確率を表す。

### MDP의 목표（MDPの目標）

$$
\pi(a|s), p(s'|s,a), r(s, a, s')
$$

위 3가지 식으로 최적의 정책(optimal policy)를 찾는 것이 MDP의 목표이다. 이때 최적 정책이란 수익이 최대가 되는 정책을 의미한다.

上記３つの要素を用いて、最適方策(optimal policy)を見つけることがMDPの目標である。ここで最適方策とは、期待収益が最大となる方策を指す。

#### 일회성 과제와 지속적 과제（エピソードタスクと連続タスク）

일회성 과제(episodic task):명확한 종료 상태가 존재하는 문제

지속적 과제(continuous task):종료 상태가 없는 문제

エピソードタスク：明確な終了状態が存在する問題

連続タスク：終了状態が存在しない問題

#### 수익(return、収益)

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2R_{t+2}+\cdots
$$

수익(\(G_t\) )은 에이전트가 얻는 보상의 합이다. 이 때 시간이 지날수록 보상은 \(\gamma\)에 의해 기하급수적으로 줄어든다. 이 

\(\gamma\)（\(0.0 < \gamma < 1.0\)）를 할인율(discount rate)라고 한다.

할인율을 도입하는 주된 이유는 지속적 과제에서 수익이 무한대가 되지 않도록 방지하기 위함이며, 가까운 미래의 보상을 더 중요하게 보이도록 하기 위해서이다.

이 수익을 극대화하는 것이 에이전트의 목표이다.

収益 \(G_t\) とは、エージェントが得る報酬の総和である。このとき、時間が経過するにつれて報酬は \(\gamma\) によって幾何級数的に減衰する。この \(\gamma\)（\(0.0 < \gamma < 1.0\)）を割引率（discount rate）という。

割引率を導入する主な理由は、継続タスクにおいて収益が無限大になるのを防ぐためであり、また近い将来の報酬をより重要視するためである。

この収益を最大化することが、エージェントの目的である。



#### 상태 가치 함수（状態価値関数）

에이전트와 환경이 '확률적'으로 동작할 수 있다는 점을 유의해야 한다.

에이전트는 확률적으로 다음 행동을 결정할 수 있고, 상태 또한 확률적으로 전이될 수 있다. 그렇다면 얻는 수익 또한, 확률적으로 달라질 수 있다.

이러한 확률적 동작에 대응하기 위해서는 기댓값, 즉 '수익의 기댓값'을 지표로 삼아야 한다.

エージェントと環境が「確率的」に動作する可能性がある点に注意する必要がある。

エージェントは確率的に次の行動を決定することができ、状態もまた確率的に遷移する。そのため、得られる収益も確率的に変化する。

このような確率的挙動に対応するためには、期待値、すなわち「収益の期待値」を指標として用いる必要がある。
$$
v_\pi(s) = E[G_t|S_t=s,\pi]
\\= E_\pi[G_t|S_t=s]
$$

#### 최적 정책과 최적 가치 함수（最適方策と最適価値関数）

두 정책의 우열을 가리려면 하나의 정책이 다른 정책보다 '모든 상태'에서 더 좋거나 최소한 똑같아야 한다.

모든 상태에서 상태 가치 함수의 값이 다른 어떤 정책보다 큰 정책이 최적 정책이다.

최적 정책의 상태 가치 함수를 최적 상태 가치 함수(optimal state-value function)라고 한다.

2つの方策の優劣を比較するためには、ある方策が別の方策よりも「すべての状態」において優れている、もしくは少なくとも同等でなければならない。

すべての状態において、状態価値関数の値が他のどの方策よりも大きい方策が最適方策である。

最適方策に対応する状態価値関数を、最適状態価値関数（optimal state-value function）という。

![2.최적 정책](/assets/images/rl/2.MDP/2.최적 정책.jpg)
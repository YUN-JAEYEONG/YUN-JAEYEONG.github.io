---
layout: post
title:  "벨만 방정식(bellman equation, ベルマン方程式)"
summary: 벨만 방정식(bellman equation, ベルマン方程式)
author: YUN JAEYEONG
date: '2026-02-01 16:00:00 +0900'
category: [강화학습, 強化学習]
typora-root-url: ../
use_math: true
#thumbnail: /assets/img/posts/code.jpg


---

<script type="text/javascript" async
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>



## 벨만 방정식(ベルマン方程式)

### 확률과 기댓값(確率と期待値)

각각의 눈이 나올 확률이 정확하게 $$\frac{1}{6}$$씩인 이상적인 주사위임을 가정하면, 각 확률은 다음 식으로 표현할 수 있다.

各目が出る確率が正確にそれぞれ$$\frac{1}{6}$$である理想的なサイコロを仮定すると、各確率は次の式で表される。


$$
p(x) = \frac{1}{6}
$$


주사위를 굴렸을 때 나올 기댓값은 다음과 같이 구할 수 있다.

サイコロを振った時に得られる期待値は、次のように計算できる。


$$
E[x] = 1\times\frac{1}{6}+2\times\frac{1}{6}+3\times\frac{1}{6}+4\times\frac{1}{6}+5\times\frac{1}{6}+6\times\frac{1}{6}\\=3.5
$$
조건부확률은 어떤 사건이 일어나는 경우에 다른 사건이 일어날 확률을 말한다. 사건 A가 일어났을 때 B가 일어날 확률은 다음과 같이 표현한다.

条件付確率とは、ある事象が起きた時に、別の事象が起きる確率を指す。

事象Aが起きた時に事象Bが起きる確率は、次のように表される。


$$
P(B|A) = \frac{P(A\cap B)}{P(A)}
$$


또한 A와 B가 동시에 일어날 확률, 즉 '동시 확률'은 다음과 같다.

また、AとBが同時に起きる確率、すなわち「同時確率」は次のように表される。


$$
P(A\cap B) = P(A)P(B|A)
$$

$$
P(A, B) = P(A)P(B|A)
$$

보상은 x와 y의 값에 의해 결정되기 때문에, 보상을 함수 $$r(x, y)$$로 나타낼 수 있다.

기댓값은 '값 X 그 값이 발생할 확률'의 합이므로, 보상의 기댓값은 다음 식으로 나타낼 수 있다.

報酬はｘとｙの値によって決定されるため、報酬を関数$$r(x, y)$$として表すことができる。

期待値は「値 X その値が生じる確率」の総和であるため、報酬の期待値は次の式で表される。
$$
E[r(x, y)] = \displaystyle\sum_{x}^{}\displaystyle\sum_{y}^{}p(x,y)r(x,y)
\\=\displaystyle\sum_{x}^{}\displaystyle\sum_{y}^{}p(x)p(y|x)r(x,y)
$$

### 벨만 방정식 도출（ベルマン方程式の導出）

#### 수익（収益）

수익은 에이전트가 얻는 보상의 합으로 마르코프 결정 과정에서 배웠다.

収益とは、エージェントが得る報酬の総和であり、マルコフ決定過程で学んだ。
$$
G_t = R_t + \gamma R_{t+1} + \gamma^2R_{t+2}+\cdots
$$


시간 t+1이후에 얻을 수 있는 보상의 합을 다음과 같이 나타내면, Gt와 Gt+1의 관계를 알 수 있다.

時刻t+1以降に得られる報酬の総和を次のように表すと、GtとGt+1の関係がわかる。


$$
G_{t+1} = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+\cdots
$$

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2R_{t+2}+\cdots
\\=R_t + \gamma(R_{t+1}+R_{t+2}+\cdots)
\\=R_t + \gamma G_{t+1}
$$

#### 상태 가치 함수（状態価値関数）

상태 가치 함수는 수익에 대한 기댓값이며, 다음 식으로 정의된다.

状態価値関数は収益の期待値であり、次のように定義される。


$$
v_\pi(s) = E_\pi[G_t|S_t=s]
$$


상태 가치 함수의 수식에 위 식을 대입해보면 다음과 같다.

この定義に先ほどの式を代入すると、次のように展開できる。


$$
v_\pi(s) = E_\pi[G_t|S_t=s]
\\=E_\pi[R_t+\gamma G_{t+1}|S_t = s]
\\=E_\pi[R_t|S_t = s]+\gamma E_\pi[G_{t+1}|S_t = s]
$$



현재 상태의 가치는 지금 얻는 즉시 보상의 기대값과, 다음 상태부터 얻을 수 있는 미래 수익의 기대값의 합으로 분해할 수 있다.

現在の状態の価値は、今得られる即時報酬の期待値と、次の状態以降に得られる将来収益の期待値の和として分解できる。


$$
E_\pi[R_t|S_t=s]=\displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)r(s, a, s')
$$

$$
E_\pi[G_{t+1}|S_t = s] = \displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)E_\pi[G_{t+1}|S_{t+1}=s']
\\=\displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)v_\pi(s')
$$

각각 전개한 항을 앞서 전개한 식에 대입하면 다음 식이 도출된다.

これらを先ほどの式に代入すると、次の式が導かれる。


$$
v_\pi(s)=E_\pi[R_t|S_t = s]+\gamma E_\pi[G_{t+1}|S_t = s]
\\=\displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)r(s, a, s') + \gamma\displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)v_\pi(s')
\\=\displaystyle\sum_{a}^{}\displaystyle\sum_{s'}^{}\pi(a|s)p(s'|s,a)\{r(s, a, s')+\gamma v_\pi(s')\}
$$
이 식을 벨만 방정식이라 한다. 벨만 방정식은 '상태 s의 상태 가치 함수'와 '다음에 취할 수 있는 상태 s'의 상태 가치 함수'의 관계를 나타낸 식으로, 모든 상태 s와 모든 정책 $$\pi$$에 대해 성립한다.

この式をベルマン方程式と呼ぶ。

ベルマン方程式は、「状態ｓにおける状態価値関数」と「次の遷移しうる状態ｓ’の状態価値関数」との関係を表しており、すべての状態ｓとすべての方策$$\pi$$に対して成り立つ。

### 행동 가치 함수 (Q 함수)와 벨만 방정식（行動価値関数（Q関数）とベルマン方程式）

#### 행동 가치 함수(action-value function, Q-function)

$$
q_\pi(s, a) = E_\pi[G_t|S_t=s, A_t=a]
$$

Q 함수는 시간이 t일 때 상태 s에서 행동 a를 취하고, 시간 t+1부터는 정책 $$\pi$$에 따라 행동을 결정하는데, 이때 얻을 수 있는 기대 수익이 $$q_\pi(s,a)$$이다. 

상태 가치 함수에서 행동 a는 정책 $$\pi$$에 따라 선택된다. 반면 Q 함수에서 행동 a는 자유롭게 선택할 수 있다.

이 때 행동 a를 정책 $$\pi$$에 따라 선택하도록 설계하면 Q 함수와 상태 가치 함수는 완전히 동일해진다.

Q関数とは、時刻ｔに状態ｓで行動aを選択し、時刻 t+1 以降は方策$$\pi$$に従って行動したときに得られる期待収益を表す。

状態価値関数では行動は方策$$\pi$$に従って選択される。一方、Q関数では行動aを自由に指定できる。

行動を方策$$\pi$$に従って選択すると、Q関数と状態価値関数は次の関係を満たす。
$$
v_\pi(s)=\displaystyle\sum_{a}^{}\pi(a|s)q_\pi(s,a)
$$

#### 행동 가치 함수를 이용한 벨만 방정식

먼저 다음과 같이 Q함수를 전개한다.

まず、Q関数を次のように展開する。


$$
q_\pi(s, a) = E_\pi[G_t|S_t=s, A_t=a]
\\= E_\pi[R_t + \gamma G_{t+1}|S_t=s, A_t=a]
\\=E_\pi[R_t|S_t=s, A_t=a] + \gamma E_\pi[G_{t+1}|S_t=s, A_t=a]
\\=\displaystyle\sum_{s'}^{}p(s'|s,a)r(s, a, s')+\gamma\displaystyle\sum_{s'}^{}p(s'|s,a)E_\pi[G_{t+1}|S_{t+1}=s']
\\=\displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s, a, s')+\gamma E_\pi[G_{t+1}|S_{t+1}=s']\}
\\=\displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s, a, s')+\gamma v_\pi(s')\}
$$


위 식을 이용하면 상태가치 함수$$v_\pi(s')$$는 다음처럼 쓸 수 있다.

この式を用いると、状態価値関数$$v_\pi(s')$$は次のように書ける。


$$
q_\pi(s, a) = \displaystyle\sum_{s'}^{}p(s'|s, a)\{r(s,a,s')+\gamma\displaystyle\sum_{a'}^{}\pi(a'|s')q_\pi(s', a')\}
$$


이때 a'은 시간 t+1에서의 행동이며, 위 식이 행동 가치 함수(Q-function)을 이용한 벨만 방정식이다.

ここでa'は時刻 t+1における行動であり、この式が行動価値関数に対するベルマン方程式である。

#### 벨만 최적 방정식(bellman optimality equation、ベルマン最適方程式)



최적 정책이란 모든 상태에서 상태 가치 함수가 최대인 정책이다. 정책이 '최적이다'라는 성질을 이용하면 벨만 방정식을 더 간단하게 표현할 수 있다.

벨만 방정식은 모든 정책에서 성립한다. 따라서 최적 정책을$$\pi_{*}$$라고 하면 다음과 같은 식이 성립한다.

最適方策とは、すべての状態において状態価値関数を最大化する方策である。
 この性質を用いることで、ベルマン方程式をさらに簡潔に表現できる。

ベルマン方程式はすべての方策に対して成り立つため、最適方策を $$\pi_*$$ とすると次が成り立つ。

##### 상태 가치 함수의 벨만 최적 방정식(状態価値関数のベルマン最適方程式)

$$
v_{*}(s) = \displaystyle\sum_{a}^{}\pi_*(a|s)\displaystyle\sum_{s'}^{}p(s'|s, a)\{r(s,a,s')+\gamma v_*(s')\}
$$



이 때 최적 정책이기 때문에 값이 최대인 행동을 100% 확률로 선택해야한다. 최댓값은 max 연산자를 사용하여 표현할 수 있다.

最適方策では、価値を最大にする行動を選択するため、次のように最大値演算で表すことができる。


$$
v_*(s) = \max_a\displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s,a,s')+\gamma v_*(s')\}
$$

##### 행동 가치 함수의 벨만 최적 방정식(行動価値関数のベルマン最適方程式)

$$
q_\pi(s,a) = \displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s,a,s')+\gamma\displaystyle\sum_{a'}^{}\pi(a'|s')q_\pi(s',a')\}
$$

위 식은 현재상태 s에서 행동 a를 취했을 떄의 즉시 보상과 다음 상태 s'에서 정책 $$\pi$$에 따라 행동했을 때의 기대 미래 가치의 합으로 Q 함수를 표현한다.

この式は、現在の状態 $s$ において行動 $a$ を取ったときの即時報酬と、次状態 $s'$ において方策 $\pi$ に従った場合の期待将来価値の和として Q 関数を表している。


$$
q_*(s,a) = \displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s,a,s')+\gamma\displaystyle\sum_{a'}^{}\pi_*(a'|s')q_*(s',a')\}
$$


최적 정책 $$\pi_*$$는 모든 상태에서 행동 가치 함수를 최대화하는 정책이다.
 이 성질을 이용하면, 정책에 대한 기댓값은 **최댓값 연산**으로 대체할 수 있다.

最適方策 $$\pi_*$$は、すべての状態で行動価値関数を最大化する方策である。この性質を用いると、方策に関する期待値は最大値演算に置き換えられる。


$$
q_*(s,a) = \displaystyle\sum_{s'}^{}p(s'|s,a)\{r(s,a,s')+\gamma \max_{a'} q_*(s',a')\}
$$



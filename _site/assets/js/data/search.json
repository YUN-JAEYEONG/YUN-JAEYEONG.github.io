[
  
  {
    "title": "벨만 방정식(bellman equation, ベルマン方程式)",
    "url": "/posts/%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D(%E3%83%99%E3%83%AB%E3%83%9E%E3%83%B3%E6%96%B9%E7%A8%8B%E5%BC%8F)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-02-01 16:00:00 +0900",
    "content": "벨만 방정식(ベルマン方程式)  확률과 기댓값(確率と期待値)  각각의 눈이 나올 확률이 정확하게 \\(\\frac{1}{6}\\)씩인 이상적인 주사위임을 가정하면, 각 확률은 다음 식으로 표현할 수 있다.  各目が出る確率が正確にそれぞれ\\(\\frac{1}{6}\\)である理想的なサイコロを仮定すると、各確率は次の式で表される。  [p(x) = \\frac{1}{6}]  주사위를 굴렸을 때 나올 기댓값은 다음과 같이 구할 수 있다.  サイコロを振った時に得られる期待値は、次のように計算できる。  \\(E[x] = 1\\times\\frac{1}{6}+2\\times\\frac{1}{6}+3\\times\\frac{1}{6}+4\\times\\frac{1}{6}+5\\times\\frac{1}{6}+6\\times\\frac{1}{6}\\\\=3.5\\) 조건부확률은 어떤 사건이 일어나는 경우에 다른 사건이 일어날 확률을 말한다. 사건 A가 일어났을 때 B가 일어날 확률은 다음과 같이 표현한다.  条件付確率とは、ある事象が起きた時に、別の事象が起きる確率を指す。  事象Aが起きた時に事象Bが起きる確率は、次のように表される。                 [P(B       A) = \\frac{P(A\\cap B)}{P(A)}]           또한 A와 B가 동시에 일어날 확률, 즉 ‘동시 확률’은 다음과 같다.  また、AとBが同時に起きる確率、すなわち「同時確率」は次のように表される。                 [P(A\\cap B) = P(A)P(B       A)]                          [P(A, B) = P(A)P(B       A)]           보상은 x와 y의 값에 의해 결정되기 때문에, 보상을 함수 \\(r(x, y)\\)로 나타낼 수 있다.  기댓값은 ‘값 X 그 값이 발생할 확률’의 합이므로, 보상의 기댓값은 다음 식으로 나타낼 수 있다.  報酬はｘとｙの値によって決定されるため、報酬を関数\\(r(x, y)\\)として表すことができる。  期待値は「値 X その値が生じる確率」の総和であるため、報酬の期待値は次の式で表される。 \\(E[r(x, y)] = \\displaystyle\\sum_{x}^{}\\displaystyle\\sum_{y}^{}p(x,y)r(x,y) \\\\=\\displaystyle\\sum_{x}^{}\\displaystyle\\sum_{y}^{}p(x)p(y|x)r(x,y)\\)  벨만 방정식 도출（ベルマン方程式の導出）  수익（収益）  수익은 에이전트가 얻는 보상의 합으로 마르코프 결정 과정에서 배웠다.  収益とは、エージェントが得る報酬の総和であり、マルコフ決定過程で学んだ。 \\(G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2}+\\cdots\\)  시간 t+1이후에 얻을 수 있는 보상의 합을 다음과 같이 나타내면, Gt와 Gt+1의 관계를 알 수 있다.  時刻t+1以降に得られる報酬の総和を次のように表すと、GtとGt+1の関係がわかる。  [G_{t+1} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3}+\\cdots]  [G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2}+\\cdots \\=R_t + \\gamma(R_{t+1}+R_{t+2}+\\cdots) \\=R_t + \\gamma G_{t+1}]  상태 가치 함수（状態価値関数）  상태 가치 함수는 수익에 대한 기댓값이며, 다음 식으로 정의된다.  状態価値関数は収益の期待値であり、次のように定義される。                 [v_\\pi(s) = E_\\pi[G_t       S_t=s]]           상태 가치 함수의 수식에 위 식을 대입해보면 다음과 같다.  この定義に先ほどの式を代入すると、次のように展開できる。                 [v_\\pi(s) = E_\\pi[G_t       S_t=s]                         \\=E_\\pi[R_t+\\gamma G_{t+1}       S_t = s]                         \\=E_\\pi[R_t       S_t = s]+\\gamma E_\\pi[G_{t+1}       S_t = s]]           현재 상태의 가치는 지금 얻는 즉시 보상의 기대값과, 다음 상태부터 얻을 수 있는 미래 수익의 기대값의 합으로 분해할 수 있다.  現在の状態の価値は、今得られる即時報酬の期待値と、次の状態以降に得られる将来収益の期待値の和として分解できる。                 [E_\\pi[R_t       S_t=s]=\\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s’}^{}\\pi(a       s)p(s’       s,a)r(s, a, s’)]                          [E_\\pi[G_{t+1}       S_t = s] = \\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s’}^{}\\pi(a       s)p(s’       s,a)E_\\pi[G_{t+1}       S_{t+1}=s’]                 \\=\\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s’}^{}\\pi(a       s)p(s’       s,a)v_\\pi(s’)]                           각각 전개한 항을 앞서 전개한 식에 대입하면 다음 식이 도출된다.  これらを先ほどの式に代入すると、次の式が導かれる。  \\(v_\\pi(s)=E_\\pi[R_t|S_t = s]+\\gamma E_\\pi[G_{t+1}|S_t = s] \\\\=\\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s'}^{}\\pi(a|s)p(s'|s,a)r(s, a, s') + \\gamma\\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s'}^{}\\pi(a|s)p(s'|s,a)v_\\pi(s') \\\\=\\displaystyle\\sum_{a}^{}\\displaystyle\\sum_{s'}^{}\\pi(a|s)p(s'|s,a)\\{r(s, a, s')+\\gamma v_\\pi(s')\\}\\) 이 식을 벨만 방정식이라 한다. 벨만 방정식은 ‘상태 s의 상태 가치 함수’와 ‘다음에 취할 수 있는 상태 s’의 상태 가치 함수’의 관계를 나타낸 식으로, 모든 상태 s와 모든 정책 \\(\\pi\\)에 대해 성립한다.  この式をベルマン方程式と呼ぶ。  ベルマン方程式は、「状態ｓにおける状態価値関数」と「次の遷移しうる状態ｓ’の状態価値関数」との関係を表しており、すべての状態ｓとすべての方策\\(\\pi\\)に対して成り立つ。  행동 가치 함수 (Q 함수)와 벨만 방정식（行動価値関数（Q関数）とベルマン方程式）  행동 가치 함수(action-value function, Q-function)                 [q_\\pi(s, a) = E_\\pi[G_t       S_t=s, A_t=a]]           Q 함수는 시간이 t일 때 상태 s에서 행동 a를 취하고, 시간 t+1부터는 정책 \\(\\pi\\)에 따라 행동을 결정하는데, 이때 얻을 수 있는 기대 수익이 \\(q_\\pi(s,a)\\)이다.  상태 가치 함수에서 행동 a는 정책 \\(\\pi\\)에 따라 선택된다. 반면 Q 함수에서 행동 a는 자유롭게 선택할 수 있다.  이 때 행동 a를 정책 \\(\\pi\\)에 따라 선택하도록 설계하면 Q 함수와 상태 가치 함수는 완전히 동일해진다.  Q関数とは、時刻ｔに状態ｓで行動aを選択し、時刻 t+1 以降は方策\\(\\pi\\)に従って行動したときに得られる期待収益を表す。  状態価値関数では行動は方策\\(\\pi\\)に従って選択される。一方、Q関数では行動aを自由に指定できる。  行動を方策\\(\\pi\\)に従って選択すると、Q関数と状態価値関数は次の関係を満たす。 \\(v_\\pi(s)=\\displaystyle\\sum_{a}^{}\\pi(a|s)q_\\pi(s,a)\\)  행동 가치 함수를 이용한 벨만 방정식  먼저 다음과 같이 Q함수를 전개한다.  まず、Q関数を次のように展開する。                 [q_\\pi(s, a) = E_\\pi[G_t       S_t=s, A_t=a]                                 \\= E_\\pi[R_t + \\gamma G_{t+1}       S_t=s, A_t=a]                                 \\=E_\\pi[R_t       S_t=s, A_t=a] + \\gamma E_\\pi[G_{t+1}       S_t=s, A_t=a]                         \\=\\displaystyle\\sum_{s’}^{}p(s’       s,a)r(s, a, s’)+\\gamma\\displaystyle\\sum_{s’}^{}p(s’       s,a)E_\\pi[G_{t+1}       S_{t+1}=s’]                 \\=\\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s, a, s’)+\\gamma E_\\pi[G_{t+1}       S_{t+1}=s’]}                         \\=\\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s, a, s’)+\\gamma v_\\pi(s’)}]                           위 식을 이용하면 상태가치 함수\\(v_\\pi(s')\\)는 다음처럼 쓸 수 있다.  この式を用いると、状態価値関数\\(v_\\pi(s')\\)は次のように書ける。                 [q_\\pi(s, a) = \\displaystyle\\sum_{s’}^{}p(s’       s, a){r(s,a,s’)+\\gamma\\displaystyle\\sum_{a’}^{}\\pi(a’       s’)q_\\pi(s’, a’)}]           이때 a’은 시간 t+1에서의 행동이며, 위 식이 행동 가치 함수(Q-function)을 이용한 벨만 방정식이다.  ここでa’は時刻 t+1における行動であり、この式が行動価値関数に対するベルマン方程式である。  벨만 최적 방정식(bellman optimality equation、ベルマン最適方程式)  최적 정책이란 모든 상태에서 상태 가치 함수가 최대인 정책이다. 정책이 ‘최적이다’라는 성질을 이용하면 벨만 방정식을 더 간단하게 표현할 수 있다.  벨만 방정식은 모든 정책에서 성립한다. 따라서 최적 정책을\\(\\pi_{*}\\)라고 하면 다음과 같은 식이 성립한다.  最適方策とは、すべての状態において状態価値関数を最大化する方策である。  この性質を用いることで、ベルマン方程式をさらに簡潔に表現できる。  ベルマン方程式はすべての方策に対して成り立つため、最適方策を \\(\\pi_*\\) とすると次が成り立つ。  상태 가치 함수의 벨만 최적 방정식(状態価値関数のベルマン最適方程式)                 [v_{}(s) = \\displaystyle\\sum_{a}^{}\\pi_(a       s)\\displaystyle\\sum_{s’}^{}p(s’       s, a){r(s,a,s’)+\\gamma v_*(s’)}]           이 때 최적 정책이기 때문에 값이 최대인 행동을 100% 확률로 선택해야한다. 최댓값은 max 연산자를 사용하여 표현할 수 있다.  最適方策では、価値を最大にする行動を選択するため、次のように最大値演算で表すことができる。                 [v_*(s) = \\max_a\\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s,a,s’)+\\gamma v_*(s’)}]           행동 가치 함수의 벨만 최적 방정식(行動価値関数のベルマン最適方程式)                 [q_\\pi(s,a) = \\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s,a,s’)+\\gamma\\displaystyle\\sum_{a’}^{}\\pi(a’       s’)q_\\pi(s’,a’)}]           위 식은 현재상태 s에서 행동 a를 취했을 떄의 즉시 보상과 다음 상태 s’에서 정책 \\(\\pi\\)에 따라 행동했을 때의 기대 미래 가치의 합으로 Q 함수를 표현한다.  この式は、現在の状態 $s$ において行動 $a$ を取ったときの即時報酬と、次状態 $s’$ において方策 $\\pi$ に従った場合の期待将来価値の和として Q 関数を表している。                 [q_*(s,a) = \\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s,a,s’)+\\gamma\\displaystyle\\sum_{a’}^{}\\pi_*(a’       s’)q_*(s’,a’)}]           최적 정책 \\(\\pi_*\\)는 모든 상태에서 행동 가치 함수를 최대화하는 정책이다.  이 성질을 이용하면, 정책에 대한 기댓값은 최댓값 연산으로 대체할 수 있다.  最適方策 \\(\\pi_*\\)は、すべての状態で行動価値関数を最大化する方策である。この性質を用いると、方策に関する期待値は最大値演算に置き換えられる。                 [q_*(s,a) = \\displaystyle\\sum_{s’}^{}p(s’       s,a){r(s,a,s’)+\\gamma \\max_{a’} q_*(s’,a’)}]"
  },
  
  {
    "title": "마르코프 결정 과정(MDP, マルコフ決定過程)",
    "url": "/posts/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EA%B2%B0%EC%A0%95-%EA%B3%BC%EC%A0%95(MDP,-%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-28 16:00:00 +0900",
    "content": "마르코프 결정 과정(MDP, マルコフ決定過程)  결정 과정（決定過程）  ‘결정 과정’이란 ‘에이전트가 (환경과 상호작용하면서) 행동을 결정하는 과정’을 뜻한다.  에이전트가 행동을 취함으로써 상태가 변하며, 보상도 달라진다.  「決定過程」とは、「エージェントが（環境と相互作用しながら）行動を決定する過程」を指す。  エージェントが行動を取ることによって状態が変化し、それに伴って報酬も変化する。   \\(S_0,A_0,R_0,S_1,S_1,R_1,S_2,A_2,R_2,\\cdots\\)  에이전트와 환경의 상호작용은 위와 같은 전이를 만들어낸다.  エージェントと環境の相互作用は、上記のような遷移を生み出す  상태전이（状態遷移）  상태 전이 함수(状態遷移関数)                 [p(s’       s,a)]           위 식을 상태 전이 확률이라고 한다. 에이전트가 상태 s에서 행동 a를 선택했을 때, 다음 상태 s’로 이동할 확률이다.  上式を状態遷移確立という。エージェントが状態sにおいて行動aを選択したとき、次の状態s’に遷移する確率を表す。  마르코프 성질（マルコフ性）  위 식에서 다음 상태 s’를 결정하는데 현재 상태와 행동인 s와 a만이 영향을 준다.  이처럼 마르코프 성질이란 현재의 정보만을 고려하는 성질이다.  “과거의 모든 이력은 현재 상태에 이미 반영되어 있다.”는 가정이다.  MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이와 보상을 모델링한다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야해서, 그 조합이 기하급수적으로 많아진다.  上式において、次の状態s’を決定するのに影響を与えるのは、現在の状態と行動であるsとaのみである。  このように、マルコフ性とは「現在の情報のみを考慮する性質」を指す。  「過去のすべての履歴は、現在の状態にすでに反映されている」という仮定である。  MDPでは、マルコフ性が成り立つと仮定して状態遷移と報酬をモデリングする。もしマルコフ性を仮定しない場合、過去のすべての状態や行動まで考慮する必要があり、その組み合わせは幾何級数的に増大してしまう。  보상 함수（報酬関数）  [r(s, a, s’)]  위 식을 보상 함수라 한다. 에이전트가 상태 s에서 행동 a를 수행하여 다음 상태가 s’가 되었을 떄 얻는 보상을 의미한다.  上式を報酬関数という。エージェントが状態sにおいて行動aを実行し、次の状態がs’になったときに得られる報酬を意味する。  에이전트의 정책（エージェントの方策）                 [\\pi(a       s)]           위 식은 상태 s에서 행동 a를 취할 확률을 나타낸다.  上式は、状態 sにおいて行動 aを取る確率を表す。  MDP의 목표（MDPの目標）                 [\\pi(a       s), p(s’       s,a), r(s, a, s’)]           위 3가지 식으로 최적의 정책(optimal policy)를 찾는 것이 MDP의 목표이다. 이때 최적 정책이란 수익이 최대가 되는 정책을 의미한다.  上記３つの要素を用いて、最適方策(optimal policy)を見つけることがMDPの目標である。ここで最適方策とは、期待収益が最大となる方策を指す。  일회성 과제와 지속적 과제（エピソードタスクと連続タスク）  일회성 과제(episodic task):명확한 종료 상태가 존재하는 문제  지속적 과제(continuous task):종료 상태가 없는 문제  エピソードタスク：明確な終了状態が存在する問題  連続タスク：終了状態が存在しない問題  수익(return、収益)  [G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2}+\\cdots]  수익(Gt)은 에이전트가 얻는 보상의 합이다. 이 때 시간이 지날수록 보상은 (\\gamma)에 의해 기하급수적으로 줄어든다. 이  γ（0.0 &lt; γ &lt; 1.0）를 할인율(discount rate)라고 한다.  할인율을 도입하는 주된 이유는 지속적 과제에서 수익이 무한대가 되지 않도록 방지하기 위함이며, 가까운 미래의 보상을 더 중요하게 보이도록 하기 위해서이다.  이 수익을 극대화하는 것이 에이전트의 목표이다.  収益 (Gt) とは、エージェントが得る報酬の総和である。このとき、時間が経過するにつれて報酬は (\\gamma) によって幾何級数的に減衰する。この γ（0.0 &lt; γ &lt; 1.0）を割引率（discount rate）という。  割引率を導入する主な理由は、継続タスクにおいて収益が無限大になるのを防ぐためであり、また近い将来の報酬をより重要視するためである。  この収益を最大化することが、エージェントの目的である。  상태 가치 함수（状態価値関数）  에이전트와 환경이 ‘확률적’으로 동작할 수 있다는 점을 유의해야 한다.  에이전트는 확률적으로 다음 행동을 결정할 수 있고, 상태 또한 확률적으로 전이될 수 있다. 그렇다면 얻는 수익 또한, 확률적으로 달라질 수 있다.  이러한 확률적 동작에 대응하기 위해서는 기댓값, 즉 ‘수익의 기댓값’을 지표로 삼아야 한다.  エージェントと環境が「確率的」に動作する可能性がある点に注意する必要がある。  エージェントは確率的に次の行動を決定することができ、状態もまた確率的に遷移する。そのため、得られる収益も確率的に変化する。  このような確率的挙動に対応するためには、期待値、すなわち「収益の期待値」を指標として用いる必要がある。 \\(v_\\pi(s) = E[G_t|S_t=s,\\pi] \\\\= E_\\pi[G_t|S_t=s]\\)  최적 정책과 최적 가치 함수（最適方策と最適価値関数）  두 정책의 우열을 가리려면 하나의 정책이 다른 정책보다 ‘모든 상태’에서 더 좋거나 최소한 똑같아야 한다.  모든 상태에서 상태 가치 함수의 값이 다른 어떤 정책보다 큰 정책이 최적 정책이다.  최적 정책의 상태 가치 함수를 최적 상태 가치 함수(optimal state-value function)라고 한다.  2つの方策の優劣を比較するためには、ある方策が別の方策よりも「すべての状態」において優れている、もしくは少なくとも同等でなければならない。  すべての状態において、状態価値関数の値が他のどの方策よりも大きい方策が最適方策である。  最適方策に対応する状態価値関数を、最適状態価値関数（optimal state-value function）という。"
  },
  
  {
    "title": "밴디트 문제(バンディット問題)",
    "url": "/posts/%EB%B0%B4%EB%94%94%ED%8A%B8-%EB%AC%B8%EC%A0%9C(%E3%83%90%E3%83%B3%E3%83%87%E3%82%A3%E3%83%83%E3%83%88%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-27 19:00:00 +0900",
    "content": "밴디트 문제(バンディット問題)  머신러닝의 분류（機械学習の分類）          지도 학습:정답이 있는 데이터를 활용해 데이터를 학습           비지도 학습:정답 레이블이 없는 데이터를 군집화 하는 학습           강화 학습:에이전트가 환경과 상호작용하면서 수집한 데이터를 바탕으로 더 많은 보상을 얻는 방법 학습      ㅤ      教師あり学習：正解ラベル付きデータを用いて学習する手法   教師なし学習：正解ラベルのないデータをクラスタリングする学習   強化学習：エージェントが環境と相互作用しながら得たデータを基に，より多くの報酬を得る方法を学習する手法   밴디트 문제（バンディット問題）  슬롯머신 = 환경  スロットマシン＝環境  플레이어 = 에이전트  プレイヤー＝エージェント  좋은 슬롯 머신이란?（良いスロットマシンとは？）  기댓값(expectation value)이 높은 슬롯 머신이 더 좋은 슬롯머신이다.  期待値が高いスロットマシンほど、いいスロットマシンである。  평균을 구하는 코드（平均を求めるコード）  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n}]  #avg.py import numpy as np  np.random.seed(0) rewards=[] for n in range(1, 11):     reward = np.random.rand()     rewards.append(reward)     Q = sum(rewards) / n     print(Q)   증분 구현(incremental implementation、インクリメンタルに計算)  [Q_{n-1} = \\frac{R_1 + R_2 + \\cdots + R_{n-1}} {n-1}]  [R_1 + R_2 + \\cdots + R_{n-1} = (n-1)Q_{n-1}]  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}(R_1+\\cdots+R_{n-1}+R_n) \\= \\frac{1}{n}{(n-1)Q_{n-1}+R_n} \\= (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n]  [Q_n = (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n \\=Q_{n-1} + \\frac{1}{n}(R_n-Q_{n-1})]    Q = 0 np.random.seed(0)  for n in range(1, 11):     reward = np.random.rand()     #Q = Q + (reward - Q) / n     Q += (reward - Q) / n     print(Q)   플레이어의 정책（プレイヤーのポリシー）  활용:지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이(탐욕 정책)  탐색:슬롯머신의 가치를 정확하게 추정하기 위해 다양한 슬롯머신을 시도  活用：これまでの実際のプレイ結果を基に、最もいいと考えられるスロットマシンを選択する（貪欲方策）  探索：スロットマシンの価値を正確に推定するため、様々なスロットマシンを試す  Epsilon-Greedy Policy  ϵ의 확률로 ‘탐색’을 하고, 1-ϵ의 확률로 ‘활용’을 하는 정책.  確率ϵで「探索」を行い、確率1-ϵで「活用」を行う方策  밴디트 알고리즘의 구현（バンディットアルゴリズムの実装）  #bandit.py import numpy as np  class Bandit:     def __init__(self, arms=10):         self.rates = np.random.rand(arms)     def play(self, arm):         rate = self.rates[arm]         if rate &gt; np.random.rand():             return 1         else:             return 0   bandit = Bandit()  for i in range(3):     print(bandit.play(0))   bandit = Bandit() Q = 0  for n in range(1, 11):     reward = bandit.play(0)     Q += (reward - Q) / n     print(Q)   bandit = Bandit() Qs = np.zeros(10) ns = np.zeros(10)  for n in range(10):     action = np.random.randint(0, 10)     reward = bandit.play(action)     ns[action] += 1     Qs[action] += (reward - Qs[action]) / ns[action]     print(Qs)   에이전트 구현（エージェントの実装）  class Agent:     def __init__(self, epsilon, action_size=10):         self.epsilon = epsilon         self.Qs = np.zeros(action_size)         self.ns = np.zeros(action_size)      def update(self, action, reward):         self.ns[action] += 1         self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)   실행해보기（実行してみる）  import matplotlib.pyplot as plt  steps = 1000 epsilon = 0.1  bandit = Bandit() agent = Agent(epsilon) total_reward = 0 total_rewards = [] rates = []  for step in range(steps):     action = agent.get_action()     reward = bandit.play(action)     agent.update(action, reward)     total_reward += reward          total_rewards.append(total_reward)     rates.append(total_reward / (step + 1))  print(total_reward)  plt.ylabel('Total reward') plt.xlabel('Steps') plt.plot(total_rewards) plt.show()  plt.ylabel('Rates') plt.xlabel('Steps') plt.plot(rates) plt.show()       #bandit_avg.py runs = 200 steps = 1000 epsilon = 0.1 all_rates = np.zeros((runs, steps)) for run in range(runs):     bandit = Bandit()     agent = Agent(epsilon)     total_reward = 0     rates = []      for step in range(steps):         action = agent.get_action()         reward = bandit.play(action)         agent.update(action, reward)         total_reward += reward         rates.append(total_reward / (step + 1))      all_rates[run] = rates  avg_rates = np.average(all_rates, axis=0)  plt.ylabel(\"Rates\") plt.xlabel('Steps') plt.plot(avg_rates) plt.show()     비정상 문제（非定常問題）  정상 문제(stationary problem):보상의 확률 분포가 변하지 않는 문제  비정상 문제(non-stationary problem):보상의 확률 분포가 변하도록 설정된 문제  定常問題：報酬の確率分布が変化しない問題  非定常問題：報酬の確率分布が時間とともに変化するように設定された問題  #non-stationary 비정상문제 class NonStatBandit:     def __init__(self, arms=10):         self.arms = arms         self.rates = np.random.rand(arms)      def play(self, arm):         rate = self.rates[arm]         self.rates += 0.1 * np.random.randn(self.arms) #노이즈 추가         if rate &gt; np.random.rand():             return 1         else:              return 0   지수 이동 평균(exponential moving average、指数移動平均)  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}R_1+\\frac{1}{n}R_2+\\cdots+\\frac{1}{n}R_n]  이 때 1/n을 각 보상에 대한 ‘가중치’로 볼 수 있다.  このとき、1/nを各報酬に対する「重み」とみなすことができる   \\(Q_n = Q_{n-1} + \\frac{1}{n}(R_n - Q_{n-1})\\)  여기서 1/n을 α(0 &lt; α &lt; 1)라는 고정값으로 바꾸면 각 보상의 가중치가 다음과 같이 변한다.  ここで1/nをα(0 &lt; α &lt; 1)という固定値に置き換えると、各報酬の重みは次のように変化する。  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1})\\)  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1}) \\\\= \\alpha Rn + (1-\\alpha)Q_{n-1}\\)  [Q_{n-1} = \\alpha R_{n-1}+(1-\\alpha)Q_{n-2}]  [Q_n = \\alpha Rn + (1-\\alpha)Q_{n-1} \\= \\alpha Rn + (1-\\alpha){\\alpha R_{n-1}+(1-\\alpha)Q_{n-2}} \\= \\alpha Rn + \\alpha(1-\\alpha)R_{n-1}+(1-\\alpha)^2Q_{n-2}]  이 전개를 n번 반복하면 다음 식이 만들어진다.  この展開をｎ回繰り返すと次の式が得られる。 \\(Q_n = \\alpha R_n + \\alpha(1-\\alpha)R_{n-1} +\\cdots+\\alpha(1-\\alpha)^{n-1}R_1+\\alpha(1-\\alpha)^{n}Q_0\\)  이처럼 가중치가 지수적으로 감소하기 때문에 이를 지수 이동 평균이라 한다.  このように重みが指数的に減少するため、これを指数移動平均と呼ぶ。  비정상 문제 풀기（非定常問題の解き方）  Agent의 추정치를 고정값 α로 갱신한다.  アージェントの推定値を固定値αを用いて更新する。  #exponential moving average / exponential weighted moving average class AlphaAgent:     def __init__(self, epsilon, alpha, actions=10):         self.epsilon = epsilon         self.Qs = np.zeros(actions)         self.alpha = alpha      def update(self, action, reward):         self.Qs[action] += (reward - self.Qs[action]) * self.alpha #고정값 α      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)"
  },
  
  {
    "title": "강화학습(強化学習)",
    "url": "/posts/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5(%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-26 19:00:00 +0900",
    "content": "강화학습(強化学習)  밑바닥부터 시작하는 딥러닝 4 책을 이용해서 강화학습에 대해 공부합니다!  ゼロから作るdeep learning 4　本を見て強化学習について勉強します！"
  }
  
]


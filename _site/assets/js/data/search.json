[
  
  {
    "title": "밴디트 문제(バンディット問題)",
    "url": "/posts/%EB%B0%B4%EB%94%94%ED%8A%B8-%EB%AC%B8%EC%A0%9C(%E3%83%90%E3%83%B3%E3%83%87%E3%82%A3%E3%83%83%E3%83%88%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-27 19:00:00 +0900",
    "content": "밴디트 문제(バンディット問題)  머신러닝의 분류（機械学習の分類）     지도 학습:정답이 있는 데이터를 활용해 데이터를 학습   비지도 학습:정답 레이블이 없는 데이터를 군집화 하는 학습        강화 학습:에이전트가 환경과 상호작용하면서 수집한 데이터를 바탕으로 더 많은 보상을 얻는 방법 학습      教師あり学習：正解ラベル付きデータを用いて学習する手法   教師なし学習：正解ラベルのないデータをクラスタリングする学習   強化学習：エージェントが環境と相互作用しながら得たデータを基に，より多くの報酬を得る方法を学習する手法   밴디트 문제（バンディット問題）  슬롯머신 = 환경  スロットマシン＝環境  플레이어 = 에이전트  プレイヤー＝エージェント  좋은 슬롯 머신이란?（良いスロットマシンとは？）  기댓값(expectation value)이 높은 슬롯 머신이 더 좋은 슬롯머신이다.  期待値が高いスロットマシンほど、いいスロットマシンである。  평균을 구하는 코드（平均を求めるコード）  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n}]  #avg.py import numpy as np  np.random.seed(0) rewards=[] for n in range(1, 11):     reward = np.random.rand()     rewards.append(reward)     Q = sum(rewards) / n     print(Q)   증분 구현(incremental implementation、インクリメンタルに計算)  [Q_{n-1} = \\frac{R_1 + R_2 + \\cdots + R_{n-1}} {n-1}]  [R_1 + R_2 + \\cdots + R_{n-1} = (n-1)Q_{n-1}]  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}(R_1+\\cdots+R_{n-1}+R_n) \\= \\frac{1}{n}{(n-1)Q_{n-1}+R_n} \\= (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n]  [Q_n = (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n \\=Q_{n-1} + \\frac{1}{n}(R_n-Q_{n-1})]    Q = 0 np.random.seed(0)  for n in range(1, 11):     reward = np.random.rand()     #Q = Q + (reward - Q) / n     Q += (reward - Q) / n     print(Q)   플레이어의 정책（プレイヤーのポリシー）  활용:지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이(탐욕 정책)  탐색:슬롯머신의 가치를 정확하게 추정하기 위해 다양한 슬롯머신을 시도  活用：これまでの実際のプレイ結果を基に、最もいいと考えられるスロットマシンを選択する（貪欲方策）  探索：スロットマシンの価値を正確に推定するため、様々なスロットマシンを試す  Epsilon-Greedy Policy  ϵ의 확률로 ‘탐색’을 하고, 1-ϵ의 확률로 ‘활용’을 하는 정책.  確率ϵで「探索」を行い、確率1-ϵで「活用」を行う方策  밴디트 알고리즘의 구현（バンディットアルゴリズムの実装）  #bandit.py import numpy as np  class Bandit:     def __init__(self, arms=10):         self.rates = np.random.rand(arms)     def play(self, arm):         rate = self.rates[arm]         if rate &gt; np.random.rand():             return 1         else:             return 0   bandit = Bandit()  for i in range(3):     print(bandit.play(0))   bandit = Bandit() Q = 0  for n in range(1, 11):     reward = bandit.play(0)     Q += (reward - Q) / n     print(Q)   bandit = Bandit() Qs = np.zeros(10) ns = np.zeros(10)  for n in range(10):     action = np.random.randint(0, 10)     reward = bandit.play(action)     ns[action] += 1     Qs[action] += (reward - Qs[action]) / ns[action]     print(Qs)   에이전트 구현（エージェントの実装）  class Agent:     def __init__(self, epsilon, action_size=10):         self.epsilon = epsilon         self.Qs = np.zeros(action_size)         self.ns = np.zeros(action_size)      def update(self, action, reward):         self.ns[action] += 1         self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)   실행해보기（実行してみる）  import matplotlib.pyplot as plt  steps = 1000 epsilon = 0.1  bandit = Bandit() agent = Agent(epsilon) total_reward = 0 total_rewards = [] rates = []  for step in range(steps):     action = agent.get_action()     reward = bandit.play(action)     agent.update(action, reward)     total_reward += reward          total_rewards.append(total_reward)     rates.append(total_reward / (step + 1))  print(total_reward)  plt.ylabel('Total reward') plt.xlabel('Steps') plt.plot(total_rewards) plt.show()  plt.ylabel('Rates') plt.xlabel('Steps') plt.plot(rates) plt.show()       #bandit_avg.py runs = 200 steps = 1000 epsilon = 0.1 all_rates = np.zeros((runs, steps)) for run in range(runs):     bandit = Bandit()     agent = Agent(epsilon)     total_reward = 0     rates = []      for step in range(steps):         action = agent.get_action()         reward = bandit.play(action)         agent.update(action, reward)         total_reward += reward         rates.append(total_reward / (step + 1))      all_rates[run] = rates  avg_rates = np.average(all_rates, axis=0)  plt.ylabel(\"Rates\") plt.xlabel('Steps') plt.plot(avg_rates) plt.show()     비정상 문제（非定常問題）  정상 문제(stationary problem):보상의 확률 분포가 변하지 않는 문제  비정상 문제(non-stationary problem):보상의 확률 분포가 변하도록 설정된 문제  定常問題：報酬の確率分布が変化しない問題  非定常問題：報酬の確率分布が時間とともに変化するように設定された問題  #non-stationary 비정상문제 class NonStatBandit:     def __init__(self, arms=10):         self.arms = arms         self.rates = np.random.rand(arms)      def play(self, arm):         rate = self.rates[arm]         self.rates += 0.1 * np.random.randn(self.arms) #노이즈 추가         if rate &gt; np.random.rand():             return 1         else:              return 0   지수 이동 평균(exponential moving average、指数移動平均)  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}R_1+\\frac{1}{n}R_2+\\cdots+\\frac{1}{n}R_n]  이 때 1/n을 각 보상에 대한 ‘가중치’로 볼 수 있다.  このとき、1/nを各報酬に対する「重み」とみなすことができる   \\(Q_n = Q_{n-1} + \\frac{1}{n}(R_n - Q_{n-1})\\)  여기서 1/n을 α(0 &lt; α &lt; 1)라는 고정값으로 바꾸면 각 보상의 가중치가 다음과 같이 변한다.  ここで1/nをα(0 &lt; α &lt; 1)という固定値に置き換えると、各報酬の重みは次のように変化する。  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1})\\)  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1}) \\\\= \\alpha Rn + (1-\\alpha)Q_{n-1}\\)  [Q_{n-1} = \\alpha R_{n-1}+(1-\\alpha)Q_{n-2}]  [Q_n = \\alpha Rn + (1-\\alpha)Q_{n-1} \\= \\alpha Rn + (1-\\alpha){\\alpha R_{n-1}+(1-\\alpha)Q_{n-2}} \\= \\alpha Rn + \\alpha(1-\\alpha)R_{n-1}+(1-\\alpha)^2Q_{n-2}]  이 전개를 n번 반복하면 다음 식이 만들어진다.  この展開をｎ回繰り返すと次の式が得られる。 \\(Q_n = \\alpha R_n + \\alpha(1-\\alpha)R_{n-1} +\\cdots+\\alpha(1-\\alpha)^{n-1}R_1+\\alpha(1-\\alpha)^{n}Q_0\\)  이처럼 가중치가 지수적으로 감소하기 때문에 이를 지수 이동 평균이라 한다.  このように重みが指数的に減少するため、これを指数移動平均と呼ぶ。  비정상 문제 풀기（非定常問題の解き方）  Agent의 추정치를 고정값 α로 갱신한다.  アージェントの推定値を固定値αを用いて更新する。  #exponential moving average / exponential weighted moving average class AlphaAgent:     def __init__(self, epsilon, alpha, actions=10):         self.epsilon = epsilon         self.Qs = np.zeros(actions)         self.alpha = alpha      def update(self, action, reward):         self.Qs[action] += (reward - self.Qs[action]) * self.alpha #고정값 α      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)"
  }
  
]


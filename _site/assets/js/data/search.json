[
  
  {
    "title": "마르코프 결정 과정(MDP, マルコフ決定過程)",
    "url": "/posts/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84-%EA%B2%B0%EC%A0%95-%EA%B3%BC%EC%A0%95(MDP,-%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-28 16:00:00 +0900",
    "content": "마르코프 결정 과정(MDP, マルコフ決定過程)  결정 과정（決定過程）  ‘결정 과정’이란 ‘에이전트가 (환경과 상호작용하면서) 행동을 결정하는 과정’을 뜻한다.  에이전트가 행동을 취함으로써 상태가 변하며, 보상도 달라진다.  「決定過程」とは、「エージェントが（環境と相互作用しながら）行動を決定する過程」を指す。  エージェントが行動を取ることによって状態が変化し、それに伴って報酬も変化する。   \\(S_0,A_0,R_0,S_1,S_1,R_1,S_2,A_2,R_2,\\cdots\\)  에이전트와 환경의 상호작용은 위와 같은 전이를 만들어낸다.  エージェントと環境の相互作用は、上記のような遷移を生み出す  상태전이（状態遷移）  상태 전이 함수(状態遷移関数)                 [p(s’       s,a)]           위 식을 상태 전이 확률이라고 한다. 에이전트가 상태 s에서 행동 a를 선택했을 때, 다음 상태 s’로 이동할 확률이다.  上式を状態遷移確立という。エージェントが状態sにおいて行動aを選択したとき、次の状態s’に遷移する確率を表す。  마르코프 성질（マルコフ性）  위 식에서 다음 상태 s’를 결정하는데 현재 상태와 행동인 s와 a만이 영향을 준다.  이처럼 마르코프 성질이란 현재의 정보만을 고려하는 성질이다.  “과거의 모든 이력은 현재 상태에 이미 반영되어 있다.”는 가정이다.  MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이와 보상을 모델링한다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야해서, 그 조합이 기하급수적으로 많아진다.  上式において、次の状態s’を決定するのに影響を与えるのは、現在の状態と行動であるsとaのみである。  このように、マルコフ性とは「現在の情報のみを考慮する性質」を指す。  「過去のすべての履歴は、現在の状態にすでに反映されている」という仮定である。  MDPでは、マルコフ性が成り立つと仮定して状態遷移と報酬をモデリングする。もしマルコフ性を仮定しない場合、過去のすべての状態や行動まで考慮する必要があり、その組み合わせは幾何級数的に増大してしまう。  보상 함수（報酬関数）  [r(s, a, s’)]  위 식을 보상 함수라 한다. 에이전트가 상태 s에서 행동 a를 수행하여 다음 상태가 s’가 되었을 떄 얻는 보상을 의미한다.  上式を報酬関数という。エージェントが状態sにおいて行動aを実行し、次の状態がs’になったときに得られる報酬を意味する。  에이전트의 정책（エージェントの方策）                 [\\pi(a       s)]           위 식은 상태 s에서 행동 a를 취할 확률을 나타낸다.  上式は、状態 sにおいて行動 aを取る確率を表す。  MDP의 목표（MDPの目標）                 [\\pi(a       s), p(s’       s,a), r(s, a, s’)]           위 3가지 식으로 최적의 정책(optimal policy)를 찾는 것이 MDP의 목표이다. 이때 최적 정책이란 수익이 최대가 되는 정책을 의미한다.  上記３つの要素を用いて、最適方策(optimal policy)を見つけることがMDPの目標である。ここで最適方策とは、期待収益が最大となる方策を指す。  일회성 과제와 지속적 과제（エピソードタスクと連続タスク）  일회성 과제(episodic task):명확한 종료 상태가 존재하는 문제  지속적 과제(continuous task):종료 상태가 없는 문제  エピソードタスク：明確な終了状態が存在する問題  連続タスク：終了状態が存在しない問題  수익(return、収益)  [G_t = R_t + \\gamma R_{t+1} + \\gamma^2R_{t+2}+\\cdots]  수익((G_t) )은 에이전트가 얻는 보상의 합이다. 이 때 시간이 지날수록 보상은 (\\gamma)에 의해 기하급수적으로 줄어든다. 이  (\\gamma)（(0.0 &lt; \\gamma &lt; 1.0)）를 할인율(discount rate)라고 한다.  할인율을 도입하는 주된 이유는 지속적 과제에서 수익이 무한대가 되지 않도록 방지하기 위함이며, 가까운 미래의 보상을 더 중요하게 보이도록 하기 위해서이다.  이 수익을 극대화하는 것이 에이전트의 목표이다.  収益 (G_t) とは、エージェントが得る報酬の総和である。このとき、時間が経過するにつれて報酬は (\\gamma) によって幾何級数的に減衰する。この (\\gamma)（(0.0 &lt; \\gamma &lt; 1.0)）を割引率（discount rate）という。  割引率を導入する主な理由は、継続タスクにおいて収益が無限大になるのを防ぐためであり、また近い将来の報酬をより重要視するためである。  この収益を最大化することが、エージェントの目的である。  상태 가치 함수（状態価値関数）  에이전트와 환경이 ‘확률적’으로 동작할 수 있다는 점을 유의해야 한다.  에이전트는 확률적으로 다음 행동을 결정할 수 있고, 상태 또한 확률적으로 전이될 수 있다. 그렇다면 얻는 수익 또한, 확률적으로 달라질 수 있다.  이러한 확률적 동작에 대응하기 위해서는 기댓값, 즉 ‘수익의 기댓값’을 지표로 삼아야 한다.  エージェントと環境が「確率的」に動作する可能性がある点に注意する必要がある。  エージェントは確率的に次の行動を決定することができ、状態もまた確率的に遷移する。そのため、得られる収益も確率的に変化する。  このような確率的挙動に対応するためには、期待値、すなわち「収益の期待値」を指標として用いる必要がある。 \\(v_\\pi(s) = E[G_t|S_t=s,\\pi] \\\\= E_\\pi[G_t|S_t=s]\\)  최적 정책과 최적 가치 함수（最適方策と最適価値関数）  두 정책의 우열을 가리려면 하나의 정책이 다른 정책보다 ‘모든 상태’에서 더 좋거나 최소한 똑같아야 한다.  모든 상태에서 상태 가치 함수의 값이 다른 어떤 정책보다 큰 정책이 최적 정책이다.  최적 정책의 상태 가치 함수를 최적 상태 가치 함수(optimal state-value function)라고 한다.  2つの方策の優劣を比較するためには、ある方策が別の方策よりも「すべての状態」において優れている、もしくは少なくとも同等でなければならない。  すべての状態において、状態価値関数の値が他のどの方策よりも大きい方策が最適方策である。  最適方策に対応する状態価値関数を、最適状態価値関数（optimal state-value function）という。"
  },
  
  {
    "title": "밴디트 문제(バンディット問題)",
    "url": "/posts/%EB%B0%B4%EB%94%94%ED%8A%B8-%EB%AC%B8%EC%A0%9C(%E3%83%90%E3%83%B3%E3%83%87%E3%82%A3%E3%83%83%E3%83%88%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-27 19:00:00 +0900",
    "content": "밴디트 문제(バンディット問題)  머신러닝의 분류（機械学習の分類）          지도 학습:정답이 있는 데이터를 활용해 데이터를 학습           비지도 학습:정답 레이블이 없는 데이터를 군집화 하는 학습           강화 학습:에이전트가 환경과 상호작용하면서 수집한 데이터를 바탕으로 더 많은 보상을 얻는 방법 학습      ㅤ      教師あり学習：正解ラベル付きデータを用いて学習する手法   教師なし学習：正解ラベルのないデータをクラスタリングする学習   強化学習：エージェントが環境と相互作用しながら得たデータを基に，より多くの報酬を得る方法を学習する手法   밴디트 문제（バンディット問題）  슬롯머신 = 환경  スロットマシン＝環境  플레이어 = 에이전트  プレイヤー＝エージェント  좋은 슬롯 머신이란?（良いスロットマシンとは？）  기댓값(expectation value)이 높은 슬롯 머신이 더 좋은 슬롯머신이다.  期待値が高いスロットマシンほど、いいスロットマシンである。  평균을 구하는 코드（平均を求めるコード）  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n}]  #avg.py import numpy as np  np.random.seed(0) rewards=[] for n in range(1, 11):     reward = np.random.rand()     rewards.append(reward)     Q = sum(rewards) / n     print(Q)   증분 구현(incremental implementation、インクリメンタルに計算)  [Q_{n-1} = \\frac{R_1 + R_2 + \\cdots + R_{n-1}} {n-1}]  [R_1 + R_2 + \\cdots + R_{n-1} = (n-1)Q_{n-1}]  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}(R_1+\\cdots+R_{n-1}+R_n) \\= \\frac{1}{n}{(n-1)Q_{n-1}+R_n} \\= (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n]  [Q_n = (1-\\frac{1}{n})Q_{n-1} + \\frac{1}{n}R_n \\=Q_{n-1} + \\frac{1}{n}(R_n-Q_{n-1})]    Q = 0 np.random.seed(0)  for n in range(1, 11):     reward = np.random.rand()     #Q = Q + (reward - Q) / n     Q += (reward - Q) / n     print(Q)   플레이어의 정책（プレイヤーのポリシー）  활용:지금까지 실제로 플레이한 결과를 바탕으로 가장 좋다고 생각되는 슬롯머신을 플레이(탐욕 정책)  탐색:슬롯머신의 가치를 정확하게 추정하기 위해 다양한 슬롯머신을 시도  活用：これまでの実際のプレイ結果を基に、最もいいと考えられるスロットマシンを選択する（貪欲方策）  探索：スロットマシンの価値を正確に推定するため、様々なスロットマシンを試す  Epsilon-Greedy Policy  ϵ의 확률로 ‘탐색’을 하고, 1-ϵ의 확률로 ‘활용’을 하는 정책.  確率ϵで「探索」を行い、確率1-ϵで「活用」を行う方策  밴디트 알고리즘의 구현（バンディットアルゴリズムの実装）  #bandit.py import numpy as np  class Bandit:     def __init__(self, arms=10):         self.rates = np.random.rand(arms)     def play(self, arm):         rate = self.rates[arm]         if rate &gt; np.random.rand():             return 1         else:             return 0   bandit = Bandit()  for i in range(3):     print(bandit.play(0))   bandit = Bandit() Q = 0  for n in range(1, 11):     reward = bandit.play(0)     Q += (reward - Q) / n     print(Q)   bandit = Bandit() Qs = np.zeros(10) ns = np.zeros(10)  for n in range(10):     action = np.random.randint(0, 10)     reward = bandit.play(action)     ns[action] += 1     Qs[action] += (reward - Qs[action]) / ns[action]     print(Qs)   에이전트 구현（エージェントの実装）  class Agent:     def __init__(self, epsilon, action_size=10):         self.epsilon = epsilon         self.Qs = np.zeros(action_size)         self.ns = np.zeros(action_size)      def update(self, action, reward):         self.ns[action] += 1         self.Qs[action] += (reward - self.Qs[action]) / self.ns[action]      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)   실행해보기（実行してみる）  import matplotlib.pyplot as plt  steps = 1000 epsilon = 0.1  bandit = Bandit() agent = Agent(epsilon) total_reward = 0 total_rewards = [] rates = []  for step in range(steps):     action = agent.get_action()     reward = bandit.play(action)     agent.update(action, reward)     total_reward += reward          total_rewards.append(total_reward)     rates.append(total_reward / (step + 1))  print(total_reward)  plt.ylabel('Total reward') plt.xlabel('Steps') plt.plot(total_rewards) plt.show()  plt.ylabel('Rates') plt.xlabel('Steps') plt.plot(rates) plt.show()       #bandit_avg.py runs = 200 steps = 1000 epsilon = 0.1 all_rates = np.zeros((runs, steps)) for run in range(runs):     bandit = Bandit()     agent = Agent(epsilon)     total_reward = 0     rates = []      for step in range(steps):         action = agent.get_action()         reward = bandit.play(action)         agent.update(action, reward)         total_reward += reward         rates.append(total_reward / (step + 1))      all_rates[run] = rates  avg_rates = np.average(all_rates, axis=0)  plt.ylabel(\"Rates\") plt.xlabel('Steps') plt.plot(avg_rates) plt.show()     비정상 문제（非定常問題）  정상 문제(stationary problem):보상의 확률 분포가 변하지 않는 문제  비정상 문제(non-stationary problem):보상의 확률 분포가 변하도록 설정된 문제  定常問題：報酬の確率分布が変化しない問題  非定常問題：報酬の確率分布が時間とともに変化するように設定された問題  #non-stationary 비정상문제 class NonStatBandit:     def __init__(self, arms=10):         self.arms = arms         self.rates = np.random.rand(arms)      def play(self, arm):         rate = self.rates[arm]         self.rates += 0.1 * np.random.randn(self.arms) #노이즈 추가         if rate &gt; np.random.rand():             return 1         else:              return 0   지수 이동 평균(exponential moving average、指数移動平均)  [Q_n = \\frac{R_1 + R_2 + \\cdots + R_n} {n} \\= \\frac{1}{n}R_1+\\frac{1}{n}R_2+\\cdots+\\frac{1}{n}R_n]  이 때 1/n을 각 보상에 대한 ‘가중치’로 볼 수 있다.  このとき、1/nを各報酬に対する「重み」とみなすことができる   \\(Q_n = Q_{n-1} + \\frac{1}{n}(R_n - Q_{n-1})\\)  여기서 1/n을 α(0 &lt; α &lt; 1)라는 고정값으로 바꾸면 각 보상의 가중치가 다음과 같이 변한다.  ここで1/nをα(0 &lt; α &lt; 1)という固定値に置き換えると、各報酬の重みは次のように変化する。  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1})\\)  \\(Q_n = Q_{n-1} + \\alpha(R_n - Q_{n-1}) \\\\= \\alpha Rn + (1-\\alpha)Q_{n-1}\\)  [Q_{n-1} = \\alpha R_{n-1}+(1-\\alpha)Q_{n-2}]  [Q_n = \\alpha Rn + (1-\\alpha)Q_{n-1} \\= \\alpha Rn + (1-\\alpha){\\alpha R_{n-1}+(1-\\alpha)Q_{n-2}} \\= \\alpha Rn + \\alpha(1-\\alpha)R_{n-1}+(1-\\alpha)^2Q_{n-2}]  이 전개를 n번 반복하면 다음 식이 만들어진다.  この展開をｎ回繰り返すと次の式が得られる。 \\(Q_n = \\alpha R_n + \\alpha(1-\\alpha)R_{n-1} +\\cdots+\\alpha(1-\\alpha)^{n-1}R_1+\\alpha(1-\\alpha)^{n}Q_0\\)  이처럼 가중치가 지수적으로 감소하기 때문에 이를 지수 이동 평균이라 한다.  このように重みが指数的に減少するため、これを指数移動平均と呼ぶ。  비정상 문제 풀기（非定常問題の解き方）  Agent의 추정치를 고정값 α로 갱신한다.  アージェントの推定値を固定値αを用いて更新する。  #exponential moving average / exponential weighted moving average class AlphaAgent:     def __init__(self, epsilon, alpha, actions=10):         self.epsilon = epsilon         self.Qs = np.zeros(actions)         self.alpha = alpha      def update(self, action, reward):         self.Qs[action] += (reward - self.Qs[action]) * self.alpha #고정값 α      def get_action(self):         if np.random.rand() &lt; self.epsilon:             return np.random.randint(0, len(self.Qs))         return np.argmax(self.Qs)"
  },
  
  {
    "title": "강화학습(強化学習)",
    "url": "/posts/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5(%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92)/",
    "categories": "강화학습, 強化学習",
    "tags": "",
    "date": "2026-01-26 19:00:00 +0900",
    "content": "강화학습(強化学習)  밑바닥부터 시작하는 딥러닝 4 책을 이용해서 강화학습에 대해 공부합니다!  ゼロから作るdeep learning 4　本を見て強化学習について勉強します！"
  }
  
]

